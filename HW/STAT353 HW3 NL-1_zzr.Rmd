---
title: 'STAT 353: Homework 3'
author: "Name here"
date: "date here"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Handwork

*My policy on handwork is that it is required for Statistics PhD students and MS in Statistics students. It is encouraged (but not required) for MS in Applied Statistics and all other students. (If you are the latter, you will not get penalized if you get these wrong ... )*

Exercises from the book: 17.1, 17.2, 18.3, 18.5

You can type your answers and submit within this file *or* you can do this work on paper and submit a scanned copy (be sure it is legible).

### 17.1

1. 
The simplest result for the model (a), (b), (c) is the metric effect.
for(a):$Y = \alpha+\beta_1X_1+\beta_2X_2$ and the metric effect is $\beta_1$
for(b):$Y = \alpha+\beta_1X_1+\beta_2X_2^2+\beta_3X_2$ and the metric effect is $\beta_1+2\beta_2X_1$
for(c):$Y = \alpha+\beta_1X_1+\beta_2X_2+\beta_3X_1X_2$ and the metric effect is $\beta_1+\beta_3X2$
for(d):$Y = exp(\alpha+\beta_1X_1+\beta_2X_2)$ the rate of return is the simplest, which is $beta_1$
for(e): the point elasticity is the simplest, which is $\beta_1$

2.
  The immediate result of increasing X1 by one unit while keeping X2 fixed is known as the metric effect. 
  The immediate result of increasing X1 by a quantity equal to its size while maintaining X2 at a steady value is the effect of a proportional change in X1. 
  The immediate result of holding X2 constant while raising X1 by one unit as a percentage of the size of Y is what is known as the rate of return. 
  The estimated percentage change in Y with an increase of 1% in X1 while holding X2 constant is known as the point elasticity. Instantaneous effects are extrapolated in non-linear connections based on the slope of the regression function in the direction of X1 at a specific point.

3.
Models (a), (b), and (c) can be estimated using ordinary least squares (OLS) regression if they are linear in their parameters and if the errors are assumed to be independently and identically distributed with equal variances. For model (d) with a positive dependent variable Y and multiplicative errors, OLS regression on the logarithm of Y with respect to X1 and X2 can be performed. For model (e) with positive Y and multiplicative errors, OLS regression on the logarithm of Y with respect to the logarithm of X1 and X2 can be performed.

### 17.2

![17.2](17.2.jpg)
### 18.3

```{r}
set.seed(1)
library(stats)
cubic_function = function (x) 100 - 5*(x/10 - 5) + (x/10 - 5)^3
# simulate x
x = runif (100, min =0, max =100)
error = rnorm (100, mean =0, sd =20)
y = cubic_function(x) + error
plot(x,y)
x_1 = seq (0, 100 , length =1000)
lines(x_1 , f(x_1), col = "blue")
fit = ksmooth(x, y, kernel = "normal", bandwidth = 5)
lines(fit, col = "red")
```
```{r}
library(locfit)
set.seed(1)
plot (x, y)
x_2 = seq (0, 100 , length =1000)
lines(x_2 , f(x_2), col = 'blue')
fit2 = loess.smooth(x, y, span = 0.5)
lines(fit2, col = "red")
```
Thus, it is apparently that the kernal regression has larger bias.



### 18.5

```{r}
set.seed(1)
span_value = seq(0.05, 0.95, 0.01)
x_2 = sort(runif(100,0,100))
y = cubic_function(x) + rnorm(100, 0, 20)
y_mu = f(x)
ASEs = rep(0, length(span_value))
for (i in 1:length(span_value)) {
  y_pred <- fitted(loess(y ~ x_2, span = span_value[i]))
  ASEs[i] <- sum (( y_pred - y_mu )^2) /100
}

plot(span_value, ASEs, type = "l")
span_value[which.min(ASEs)]
```
The minimum ASE is at s = 0.05. No, it did not confirm.


## Data analysis

```{r}
library(faraway)
library(mgcv)
library(arm)
library(MASS)
library(brant)
library("car") # load car and carData packages
```

### **1. Exercise D17.1** 

The data in `ginzberg.txt` (collected by Ginzberg) were analyzed by Monette (1990). The data are for a group of 82 psychiatric patients hospitalized for depression. The response variable in the data set is the patient's score on the Beck scale, a widely used measure
of depression. The explanatory variables are "simplicity" (measuring the degree to which the patient "sees the world in black and white") and "fatalism". (These three variables have been adjusted for other explanatory variables that can influence depression.)

Using the full quadratic regression model
$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_1^2 + \beta_4X_2^2 + \beta_5X_1X_2 + \epsilon$
regress the Beck-scale scores on simplicity and fatalism.

(a) Are the quadratic and product terms needed here?

```{r}
model <- lm(depression ~ simplicity + fatalism + I(simplicity^2) + I(fatalism^2) + I(simplicity * fatalism), data=Ginzberg)
summary(model)
```

It seems that the quadratic term is not needed but the quadratic term is useful.
We use anova to test it.
```{r}
reduced_model <- lm(depression ~ simplicity + fatalism + I(simplicity^2) + I(fatalism^2), data=Ginzberg)
anova(reduced_model, model)
```
Because the p-value is smaller than 0.05, we can say that the interaction term is needed.

```{r}
reduced_model_2 <- lm(depression ~ simplicity + fatalism + I(simplicity * fatalism), data=Ginzberg)
anova(reduced_model_2, model)
```

As the p-value is significantly larger than 0.05, we can say that we donot need the quadratic term.

(b) If you have access to suitable software, graph the data and the fitted regression surface in three dimensions. Do you see any problems with the data?

```{r}

```

(c) What do standard regression diagnostics for influential observations show?

```{r}

```

### **2. Exercise D18.2** 


For this analysis, use the `States.txt` data, which includes average SAT scores for each state as the outcome.

(a) Put together a model with SAT math (`SATM`) as the outcome and `region`, `pop`, `percent`, `dollars`, and `pay` as the explanatory variables, each included as linear terms. Interpret the findings.

```{r}
model <- lm(SATM ~ region + pop + percent + dollars + pay, data = States)
summary(model)
```

In the summary table, we can clearly observe that the percent variable is significant. Thus, we can furtherly fit the model with only the percent variable. If the percent increases by 1, the average SAT scores will decrease by 1.282.

```{r}
model_2 <- lm(SATM ~ percent, data = States)
summary(model_2)
```

(b) Now, instead approach building this model using the nonparametric-regression methods of this chapter. Fit a general nonparametric regression model *and* an additive-regression model, comparing the results to each other and to the linear least-squares fit to the data (in (a)).

```{r}
#Load the states data
# local polynomial regression
stat = States
# Fit a first-degree polynomial regression model using "percent" as the only predictor
model_percent <- loess(SATM~percent, degree=1, data=stat)

# Plot the SATM vs. "percent" data and the predicted values from the "model_percent" model
plot(SATM~percent, data=stat)
increment <- with(stat, seq(min(percent), max(percent), len=100)) # 100 x-values
preds <- predict(model_percent, data.frame(percent=increment)) # fitted values
lines(increment, preds, lty=2, lwd=2, col="green") # loess curve
lines(with(stat, smooth.spline(percent, SATM, df=3.85), lwd=2), col='blue')
abline(lm(SATM~percent, data=stat), col="red")

# Fit an additive regression model (GAM)
gam_model <- gam(SATM~s(pop)+s(percent)+s(pay), data=stat)
summary(gam_model)
plot(gam_model)
```

The plot shows that both the local regression and the smoothing spline fit the data better than linear regression. The local regression and smoothing spline models appear to be subjective choices as to which is a better fit for the data. The additive-regression model is fit using a GaM model and it appears to fit the data well. The GaM model explains a significant amount of the deviance and shows that percentTaking and teacherPay are significant for both the non-parametric models. The nonlinear terms of these two variables are also plotted and it is evident that there is some nonlinearity present. Although it is possible to test for nonlinearity by fitting a linear model and comparing it through a Likelihood ratio test, it is deemed unnecessary as the plots clearly demonstrate the presence of nonlinearity.

(c) Can you handle the nonlinearity by a transformation or by another parametric regression model, such as a polynomial regression? Investigate and explain. What are the tradeoffs between these nonparametric and parametric approaches?

```{r}
pairs(stat[,2:7]) 
model_ploy <- lm(SATM~log(pop)+I(percent^2)+pay, data=stat)

# plot
ix <- sort(stat$percent, index.return=T)$ix
pred <- predict(lm(SATM~I(percent^2), data=stat))
plot(SATM~percent, data=stat)
lines(stat$percent[ix], pred[ix], col='red', lwd=2)
```

We will try to model the relationship between the 'SATM' score and the variable 'percent' using a polynomial regression model with a second-degree term, as the relationship between 'percent' and 'SATM' score seems to be non-linear. Additionally, we will apply the logarithmic transformation to the population variable, as the scatterplot indicates that there is a concentration of values near small population values. The 'region' variable will not be considered in the model.

### **3. Exercise D18.3**

Return to the `Chile.txt` dataset used in HW2. Reanalyze the data employing generalized nonparametric regression (including generalized additive) models.

(a) What, if anything, do you learn about the data from the nonparametric regression?

```{r}
# Load the data
ch = Chile
ch = ch[complete.cases(ch), ]
ch = ch[ch$vote == "Y" | ch$vote == "N",]

i = 1
while (i <= nrow(ch)) {
if(ch$vote[i] == "N"){
ch$result[i] = 0
}
else{
ch$result[i] = 1
}
i = i + 1
}

# GAM model
Generalized_Additive_Model = gam(result ~ region + sex + education + income +
s(population) + s(age) + s(statusquo), family = binomial, data = ch)
summary(Generalized_Additive_Model)
plot(Generalized_Additive_Model)

# Status quo as a linear term
Generalized_Additive_Model_Linear_StatusQuo <- gam(result ~ region + sex + education + income + s(population)
+s(age) + statusquo, family = binomial, data = ch)

#compare
anova(Generalized_Additive_Model_Linear_StatusQuo, Generalized_Additive_Model, test = "Chisq")

#Nonparametric

# loess
Loess_Model = loess(result ~ statusquo, degree = 1, data = ch)
increment = with(ch, seq(min(statusquo), max(statusquo), len = 100))
predictions_loess = predict(Loess_Model, data.frame(statusquo = increment))

# regular logistic regression
Logistic_Regression_Model = glm(result ~ statusquo, family = binomial, data = ch)
predicted_dat = data.frame(statusquo = seq(min(ch$statusquo), max(ch$statusquo),len = 100))
predicted_dat$result = predict(Logistic_Regression_Model, predicted_dat, type = "response")

plot(result ~ statusquo, data = ch)
lines(with(ch, smooth.spline(statusquo, result, df = 3.85), lwd = 2),col = "blue")
lines(result ~ statusquo, predicted_dat, lwd = 2, col = "red")
```

The summary of our GAM model reveals that the coefficient estimates indicate that the variables of sex and education have a substantial impact on the outcome. Although there is a non-linear relationship, which means that its impact on the result is not just linear, the status quo variable also seems to be substantial. But when we plot the smooth terms, we can see that the non-linear relationship is not very strong. We fit a model with simply a linear relationship for the status quo to test for non-linearity in order to ascertain this, and we discover that the non-linear element is not required. This implies that the amount of information that can be gleaned via non-parametric approaches is limited.

We exclusively concentrate on the status quo variable, the only continuous variable of significance, in our non-additive approach. We use both a loess line and a logistic regression curve to visualize the data, which has a sigmoidal form. Given that it is steeper than the Loess curve, the logistic regression curve seems to represent the data more accurately.

(b) If the results appear to be substantially nonlinear, can you deal with the nonlinearity in a suitably respecified generalized linear model (e.g., by transforming one or more explanatory variables)?

```{r}
hist(ch$statusquo)

log_model <- glm(result ~ log(statusquo + 2), family = binomial, data = ch)
sqrt_model <- glm(result ~ sqrt(statusquo + 2), family = binomial, data = ch)
sq_model <- glm(result ~ I(statusquo^2), family = binomial, data = ch)

summary(log_model)
summary(sqrt_model)
summary(sq_model)

log_prediction_data <- data.frame(statusquo = seq(min(ch$statusquo), max(ch$statusquo), len = 100))
sqrt_prediction_data <- data.frame(statusquo = seq(min(ch$statusquo), max(ch$statusquo), len = 100))
sq_prediction_data <- data.frame(statusquo = seq(min(ch$statusquo), max(ch$statusquo), len = 100))

log_prediction_data$result <- predict(log_model, log_prediction_data, type = "response")
sqrt_prediction_data$result <- predict(sqrt_model, sqrt_prediction_data, type = "response")
sq_prediction_data$result <- predict(sq_model, sq_prediction_data, type = "response")

plot(result ~ statusquo, data = ch)
lines(result ~ statusquo, predicted_dat, lwd = 2, col = "red")
lines(result ~ statusquo, log_prediction_data, lwd = 2, col = "blue")
lines(result ~ statusquo, sqrt_prediction_data, lwd = 2, col = "green")
lines(result ~ statusquo, sq_prediction_data, lwd = 2, col = "purple")
```

We will examine the relationship between the outcome variable and the continuous variable "statusquo." We attempt three different transformations on the data: taking the log, taking the square root (while shifting the variable by 2 to prevent missing values), and taking the square. We compare the predicted curves from these transformations to the curve from a linear logistic regression model without transforming the data. The curve from the linear logistic regression is shown in red, the curve from the log transformation is in blue, the curve from the square root transform is in green, and the curve from taking the square of statusquo is in purple. The graph shows that taking the square of statusquo is not a suitable transformation. However, there is very little difference between the regular logistic regression and the log and square root transformations, indicating that there may not be much difference between these models.

### **4. Exercise E18.7**

For this analysis, use the `Duncan.txt` data. Here we are interested in the outcome `prestige` and the explanatory variable `income`.

(a) Fit the local-linear regression of prestige on income with span $s = 0.06$ (see Figure 18.7 in the book). This has 5.006 equivalent degrees of freedom, very close to the number of degrees of freedom for a fourth order polynomial.

```{r}
local_model <- loess(prestige~income, span=0.6, degree=1, data=Duncan)
divided_100 <- with(Duncan, seq(min(income), max(income), len=100))
pres<-predict(local_model, data.frame(income=divided_100))
plot(lnc_100, pres)
```
The plot shows the correlation between income and prestige, and a local-linear regression analysis has been performed. The relationship appears to be linear, suggesting that using local regression might not be necessary in this situation.

(b) Fit a fourth order polynomial of the data and compare the resulting regression curve with the local-linear regression.

```{r}
set.seed(1)
model_poly_dun <- lm(prestige~I(income*4), data=Duncan)

ix<-sort(Duncan$income, index.return=T)$ix
pred<-predict(model_poly_dun)
plot(prestige~income, data=Duncan)
lines(Duncan$income[ix], pred[ix], col="red", lwd=2)
```
The plot depicts the results of a fourth-degree polynomial regression model of the relationship between income and prestige. The fit of the data by this model seems to be worse compared to the fit by the local regression model. It is probable that the model has overfitted the data, and thus, a linear regression model may be sufficient in this scenario.

