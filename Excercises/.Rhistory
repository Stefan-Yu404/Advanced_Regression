UN_complete <- UN[complete.cases(UN), ]
# EDA
scatterplotMatrix(UN_complete[,-1])
UN <- read.table("Datasets/UnitedNations.txt")
UN <- UN[complete.cases(UN), ]
# EDA
scatterplotMatrix(UN[,-1])
# From the plot above GDP appears to need a log transformation
model11 <- lm(lifeFemale~. -GDPperCapita-lifeMale+log(GDPperCapita), data=complete_UN)
UN <- read.table("Datasets/UnitedNations.txt")
UN <- UN[complete.cases(UN), ]
# EDA
scatterplotMatrix(UN[,-1])
# From the plot above GDP appears to need a log transformation
model11 <- lm(lifeFemale~. -GDPperCapita-lifeMale+log(GDPperCapita), data=UN)
# Check LR test for significant variables
Anova(model11)
# Based on the P-value we have our Refined model
model12 <- lm(lifeFemale~region+tfr+infantMortality+economicActivityFemale, data=UN)
S(model12)
crPlots(model12)
outlierTest(model12)
residualPlots(model12)
UN <- read.table("Datasets/UnitedNations.txt", header = TRUE)
m <- 5
seed <- 353
# Perform multiple imputations using "mice"
UN_imputed <- mice(UN, m = m, seed = seed, maxit=20, printFlage=FALSE)
# Load the packages
library(boot)
library(car)
library(tidyverse)
library(sampleSelection)
library(caret)
library(mice)
install.packages("mice")
# Load the packages
library(boot)
library(car)
library(tidyverse)
library(sampleSelection)
library(caret)
library(mice)
UN <- read.table("Datasets/UnitedNations.txt", header = TRUE)
m <- 5
seed <- 353
# Perform multiple imputations using "mice"
UN_imputed <- mice(UN, m = m, seed = seed, maxit=20, printFlage=FALSE)
models <- with(UN_imputed, {
lm(lifeFemale ~ region + tfr + contraception + educationMale +
educationFemale + infantMortality + log(GDPperCapita) +
economicActivityMale + economicActivityFemale +
illiteracyMale + illiteracyFemale)
})
pooled_model <- pool(models)
summary(pooled_model)
## refine the model
refined_imputed_models <- with(UN_imputed, {
lm(lifeFemale~region+tfr+infantMortality+economicActivityFemale)
})
refined_pooled_models <- pool(refined_imputed_models)
summary(refined_pooled_models)
phd <- read.table("Datasets/Long-PhDs.txt", header = TRUE)
phd_ols <- phd
# Replace missing values in the "job" column with 1
phd_ols$job[is.na(phd_ols$job)] <- 1.00
model21 <- lm(job ~ gender + phd + mentor + fellowship + articles + citations, data = phd_ols)
# Display the summary of the OLS model
summary(phd_ols)
model21_reduced = lm(job ~ phd + fellowship + citations, data = phd_ols)
anova(model21_reduced, phd_ols)
phd <- read.table("Datasets/Long-PhDs.txt", header = TRUE)
phd_ols <- phd
# Replace missing values in the "job" column with 1
phd_ols$job[is.na(phd_ols$job)] <- 1.00
model21 <- lm(job ~ gender + phd + mentor + fellowship + articles + citations, data = phd_ols)
# Display the summary of the OLS model
summary(phd_ols)
model21_reduced = lm(job ~ phd + fellowship + citations, data = phd_ols)
anova(model21_reduced, model21)
model22 = model21_reduced
summary(model21_reducedl)
model22 = model21_reduced
summary(model21_reduced)
phd_heck = phd
phd_heck <- mutate(phd_heck, lfp = if_else(job > 1, "1", "0"))
phd_heck$lfp[is.na(phd_heck$lfp)] = 0
model23 = selection( lfp ~ phd + fellowship + citations,
job ~ phd + fellowship + citations, data=phd_heck)
summary(model23)
# Load the packages
library(boot)
library(car)
library(tidyverse)
library(sampleSelection)
library(caret)
library(mice)
library(censReg)
install.packages("censReg")
# Load the packages
library(boot)
library(car)
library(tidyverse)
library(sampleSelection)
library(caret)
library(mice)
library(censReg)
phd_tobit = phd
phd_tobit$job[is.na(phd_tobit$job)] = 1
model24 <- censReg(job ~ phd + fellowship + citations, left = 1, data = phd_tobit)
summary(model24)
phd_heck <- phd
phd_heck <- mutate(phd_heck, lfp = if_else(job > 1, "1", "0"))
phd_heck$lfp[is.na(phd_heck$lfp)] <-  0
model23 <- selection(lfp ~ gender + phd + mentor + fellowship + articles + citations,
job ~ gender + phd + mentor + fellowship + articles + citations, data = phd_heck)
summary(model23)
phd_tobit = phd
phd_tobit$job[is.na(phd_tobit$job)] = 1
model24 <- censReg(job ~ gender + phd + mentor + fellowship + articles + citations, left = 1, right = Inf, data = phd_tobit)
summary(model24)
phd_heck <- phd
phd_heck <- mutate(phd_heck, lfp = if_else(job > 1, "1", "0"))
phd_heck$lfp[is.na(phd_heck$lfp)] <-  0
model23 <- selection(lfp ~ phd + fellowship + citations,
job ~ phd + fellowship + citations, data = phd_heck)
summary(model23)
phd_tobit = phd
phd_tobit$job[is.na(phd_tobit$job)] = 1
model24 <- censReg(job ~ phd + fellowship + citations, left = 1, right = Inf, data = phd_tobit)
summary(model24)
compareCoefs(model22, model23, mode24l)
compareCoefs(model22, model23, mode24)
compareCoefs(model22, model23, model24)
data(Boston, package = "MASS")
??Boston
mu_hat <- mean(Boston$medv)
mu_hat
n<-sum(!is.na(Boston$medv))
medv_sd <- sd(Boston$medv)
medv_sde <- medv_sd/(sqrt(n))
medv_sde
mean_func <- function(data, i) {
return(mean(data[i]))
}
set.seed(353)
boot_means <- boot(data = Boston$medv, statistic = mean_func, R = 100000)
boot_sd <- sd(boot_means$t)
boot_sd
median(Boston$medv)
medv_median <- median(Boston$medv)
medv_median
median_func <- function(data, i) {
return(median(data[i]))
}
set.seed(353)
boot_medians <- boot(data = Boston$medv, statistic = median_func, R = 100000)
boot_medianSE <- sd(boot_medians$t)
boot_medianSE
BP <- read.table("data/BaseballPitchers.txt", header = TRUE)
BP <- read.table("Datasets/BaseballPitchers.txt", header = TRUE)
BP <- BP[, 3:18]
model41 <- lm(salary ~ ., data = BP)
base <- lm(salary ~ 1, data = BP)
model42 <- step(base, scope = list(lower = base, upper = model41), direction = "both", trace = TRUE)
BP <- read.table("Datasets/BaseballPitchers.txt", header = TRUE)
BP <- BP[, 3:18]
model41 <- lm(salary ~ ., data = BP)
base <- lm(salary ~ 1, data = BP)
model42 <- step(base, scope = list(lower = base, upper = model41), direction = "both", trace = TRUE)
summary(model42)
model43 <- lm(salary ~ years + careerERA + IP86 + careerSV + G86, data = BP)
fitted_values <- final_model$fitted.values
model43 <- lm(salary ~ years + careerERA + IP86 + careerSV + G86, data = BP)
fitted_values <- model43$fitted.values
BP <- BP[complete.cases(BP),]
rmse <- sqrt(mean((fitted_values - BP1$salary)^2))
model43 <- lm(salary ~ years + careerERA + IP86 + careerSV + G86, data = BP)
fitted_values <- model43$fitted.values
BP <- BP[complete.cases(BP),]
rmse <- sqrt(mean((fitted_values - BP$salary)^2))
rmse
set.seed(353)
# Cross-Validation
cv <- trainControl(method = "cv", number = 10)
model43 <- train(salary ~ years + careerERA + IP86 + careerSV + G86, data = BP, method = "lm", trControl = cv)
model43
n<-sum(!is.na(Boston$medv))
medv_sd <- sd(Boston$medv)
medv_sd <- medv_sd/(sqrt(n))
medv_sd
set.seed(353)
# Cross-Validation
cv <- trainControl(method = "cv", number = 10)
model43 <- train(salary ~ years + careerERA + IP86 + careerSV + G86, data = BP, method = "lm", trControl = cv)
model43
BP <- read.table("Datasets/BaseballPitchers.txt", header = TRUE)
BP <- BP[, 3:18]
model41 <- lm(salary ~ ., data = BP)
base <- lm(salary ~ 1, data = BP)
model42 <- step(base, scope = list(lower = base, upper = model41), direction = "both", trace = TRUE)
summary(model42)
pred1<-predict(model41,BP)
sse<-mean((BP$salary-pred1)^2)
sqrt(sse)
pred<-predict(model42,BPl)
pred1<-predict(model41,BP)
sse<-mean((BP$salary-pred1)^2)
sqrt(sse)
pred<-predict(model42,BP)
sse<-mean((BP$salary-pred)^2)
sqrt(sse)
final_model <- lm(salary ~ years + careerERA + IP86 + careerSV + G86, data = BP)
fitted_values <- final_model$fitted.values
BP <- BP[complete.cases(BP),]
# Calculate the root mean squared error between fitted values and actual salaries
rmse <- sqrt(mean((fitted_values - BP$salary)^2))
rmse
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(lme4)
library(car)
library(effects)
library(tidyverse)
library(lmerTest)
install.packages("lmerTest")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(lme4)
library(car)
library(effects)
library(tidyverse)
library(lmerTest)
knitr::opts_chunk$set(echo = TRUE)
# Load Packages
library(ggplot2)
library(lme4)
library(car)
library(effects)
library(tidyverse)
library(lmerTest)
# Fit the one-way random-effects ANOVA
model <- lmer(test ~ (1 | school), data = data)
knitr::opts_chunk$set(echo = TRUE)
# Load Packages
library(ggplot2)
library(lme4)
library(car)
library(effects)
library(tidyverse)
library(lmerTest)
data <- read.table("data/Snijders.txt", header = TRUE)
data <- read.table("Datasets/Snijders.txt", header = TRUE)
data <- na.omit(data)
num_rows <- nrow(data)
data$SES_c <- data$ses - mean(data$ses)
data$IQ_c <- data$iq - mean(data$iq)
set.seed(353)
schools <- sample(unique(data$school), 20)
par(mfrow = c(5, 4), mar = c(2.5, 2.5, 1, 1))
for (i in 1:length(schools)) {
school_data <- data[data$school == schools[i], ]
plot(school_data$SES_c, school_data$test, main = paste("School", schools[i]),
xlab = "SES_c", ylab = "Test Score")
abline(lm(test ~ SES_c, data = school_data))
plot(school_data$IQ_c, school_data$test, main = paste("School", schools[i]),
xlab = "IQ_c", ylab = "Test Score")
abline(lm(test ~ IQ_c, data = school_data))
}
model <- lmer(test ~ SES_c + IQ_c + (1 + SES_c + IQ_c | school), data=Snijders)
model <- lmer(test ~ SES_c + IQ_c + (1 + SES_c + IQ_c | school), data=data)
coefs <- coef(model)$school
plot(data$meanses[match(unique(data$school), data$school)], coefs$`(Intercept)`, xlab = "Mean SES", ylab = "Intercept")
plot(data$meaniq[match(unique(data$school), data$school)], coefs$`(Intercept)`, xlab = "Mean IQ", ylab = "Intercept")
plot(data$class.size[match(unique(data$school), data$school)], coefs$`(Intercept)`, xlab = "Class Size", ylab = "Intercept")
plot(data$meaniq[match(unique(data$school), data$school)], coefs$SES_c, xlab = "Mean SES", ylab = "Slope for IQ_c")
plot(data$meanses[match(unique(data$school), data$school)], coefs$SES_c, xlab = "Class Size", ylab = "Slope for SES_c")
plot(data$meaniq[match(unique(data$school), data$school)], coefs$IQ_c, xlab = "Mean SES", ylab = "Slope for IQ_c")
plot(data$meanses[match(unique(data$school), data$school)], coefs$IQ_c, xlab = "Class Size", ylab = "Slope for SES_c")
plot(data$class.size[match(unique(data$school), data$school)], coefs$SES_c, xlab = "Class Size", ylab = "Slope for SES_c")
plot(data$class.size[match(unique(data$school), data$school)], coefs$IQ_c, xlab = "Class Size", ylab = "Slope for IQ_c")
# Fit the one-way random-effects ANOVA
model <- lmer(test ~ (1 | school), data = data)
vc <- VarCorr(model)
vc
icc <- 4.2743^2 / (4.2743^2 + sigma(model)^2)
icc
model <- lmer(test ~ SES_c + IQ_c + (1 + SES_c + IQ_c | school), data = data)
summary(model)
null_model <- lmer(test ~ SES_c + IQ_c + (1 | school), data = data)
anova(null_model, model)
null_model = lmer(test ~ SES_c+IQ_c + (1 + IQ_c| school), data = data)
anova(null_model, model)
null_model = lmer(test ~ SES_c + IQ_c + (1 + SES_c| school), data = data)
anova(null_model, model)
final_model = lmer(test ~ SES_c + IQ_c + (1 + IQ_c| school), data = data)
summary(final_model)
m2 <- lmer(test ~ SES_c + IQ_c + meanses + meaniq + class.size + IQ_c:meanses + IQ_c:meaniq + IQ_c * class.size +
(IQ_c | school) + (1 | school), data = data)
summary(m2)
m3 <- lmer(test ~ SES_c + IQ_c + meanses + meaniq + IQ_c:meanses + IQ_c:meaniq + (IQ_c | school) + (1 | school), data = data)
summary(m3)
final_model <- lmer(test ~ SES_c + IQ_c +  meaniq  + (IQ_c | school) + (1 | school), data = data)
summary(final_model)
data$high_pass <- ifelse(data$test > 52, 1, 0)
model <- lmer(high_pass ~ 1 + (1 | school), data = data)
icc <- as.numeric(VarCorr(model)$school) / (as.numeric(VarCorr(model)$school) + attr(VarCorr(model), "sc")^2)
icc
data$high_pass <- ifelse(data$test > 52, 1, 0)
model <- glmer(high_pass ~ 1 + (1 | school), data = data)
vc <- VarCorr(model)
vc
icc <- 0.066921^2 / (0.066921^2 + sigma(model)^2)
icc
model <- glmer(high_pass ~ SES_c + IQ_c + (1 + SES_c + IQ_c | school), family = binomial, data = data)
model <- glmer(high_pass ~ SES_c + IQ_c + (1 + SES_c + IQ_c | school), family = binomial, data = data)
Summary(model)
model <- glmer(high_pass ~ SES_c + IQ_c + (1 + SES_c + IQ_c | school), family = binomial, data = data)
model <- glmer(high_pass ~ SES_c + IQ_c + (1 + SES_c + IQ_c | school), family = binomial, data = data)
Summary(model)
model <- glmer(high_pass ~ SES_c + IQ_c + (1 + SES_c + IQ_c | school), family = binomial, data = data)
S(model)
null_model <- glmer(high_pass ~ SES_c + IQ_c + (1 | school), family = binomial, data = data)
anova(model, null_model)
with_leve_two <- glmer(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size +  (1 | school), family = binomial, data = data)
null_model = glm(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size , family = binomial, data = data)
anova(with_leve_two, null_model)
with_leve_two <- glmer(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size +  (1 | school), data = data)
null_model = glm(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size ,  data = data)
anova(with_leve_two, null_model)
with_leve_two <- glmer(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size +  (1 | school), data = data)
null_model = glm(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size , family = binomial, data = data)
anova(with_leve_two, null_model)
S(with_leve_two)
with_leve_two <- glmer(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size +  (1 | school), family = binomial, data = data)
null_model = glm(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size , family = binomial, data = data)
anova(with_leve_two, null_model)
S(with_leve_two)
with_leve_two <- glmer(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size +  (1 | school), data = data)
null_model = glm(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size , family = binomial, data = data)
anova(with_leve_two, null_model)
S(with_leve_two)
reduced = glmer(high_pass ~ SES_c + IQ_c + meaniq +
(1 | school), family = binomial, data = data)
anova(reduced, with_leve_two)
reduced = glmer(high_pass ~ SES_c + IQ_c + meaniq +
(1 | school), data = data)
anova(reduced, with_leve_two)
with_leve_two <- glmer(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size +  (1 | school), family = binomial, data = data)
null_model = glm(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size , family = binomial, data = data)
anova(with_leve_two, null_model)
S(with_leve_two)
S(with_leve_two)
null_model <- glmer(high_pass ~ SES_c + IQ_c + (1 | school), family = binomial, data = data)
anova(model, null_model)
model <- glmer(high_pass ~ SES_c + IQ_c + (1 + SES_c + IQ_c | school), family = binomial, data = data)
S(model)
null_model <- glmer(high_pass ~ SES_c + IQ_c + (1 | school), family = binomial, data = data)
anova(model, null_model)
with_leve_two <- glmer(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size +  (1 | school), family = binomial, data = data)
null_model = glm(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size , family = binomial, data = data)
anova(with_leve_two, null_model)
S(with_leve_two)
reduced = glmer(high_pass ~ SES_c + IQ_c + meaniq +
(1 | school), family = binomial, data = data)
anova(reduced, with_leve_two)
reduced = glmer(high_pass ~ SES_c + IQ_c + meaniq +
(1 | school), family = binomial, data = data)
anova(reduced, with_leve_two)
model <- glmer(high_pass ~ SES_c + IQ_c + (1 + SES_c + IQ_c | school), family = binomial, data = data)
S(model)
data <- read.table("Datasets/Snijders.txt", header = TRUE)
data <- na.omit(data)
num_rows <- nrow(data)
data$SES_c <- data$ses - mean(data$ses)
data$IQ_c <- data$iq - mean(data$iq)
schools <- sample(unique(data$school), 20)
par(mfrow = c(5, 4), mar = c(2.5, 2.5, 1, 1))
for (i in 1:length(schools)) {
school_data <- data[data$school == schools[i], ]
plot(school_data$SES_c, school_data$test, main = paste("School", schools[i]),
xlab = "SES_c", ylab = "Test Score")
abline(lm(test ~ SES_c, data = school_data))
plot(school_data$IQ_c, school_data$test, main = paste("School", schools[i]),
xlab = "IQ_c", ylab = "Test Score")
abline(lm(test ~ IQ_c, data = school_data))
}
model <- lmer(test ~ SES_c + IQ_c + (1 + SES_c + IQ_c | school), data=data)
coefs <- coef(model)$school
plot(data$meanses[match(unique(data$school), data$school)], coefs$`(Intercept)`, xlab = "Mean SES", ylab = "Intercept")
plot(data$meaniq[match(unique(data$school), data$school)], coefs$`(Intercept)`, xlab = "Mean IQ", ylab = "Intercept")
plot(data$class.size[match(unique(data$school), data$school)], coefs$`(Intercept)`, xlab = "Class Size", ylab = "Intercept")
plot(data$meaniq[match(unique(data$school), data$school)], coefs$SES_c, xlab = "Mean SES", ylab = "Slope for IQ_c")
plot(data$meanses[match(unique(data$school), data$school)], coefs$SES_c, xlab = "Class Size", ylab = "Slope for SES_c")
plot(data$meaniq[match(unique(data$school), data$school)], coefs$IQ_c, xlab = "Mean SES", ylab = "Slope for IQ_c")
plot(data$meanses[match(unique(data$school), data$school)], coefs$IQ_c, xlab = "Class Size", ylab = "Slope for SES_c")
plot(data$class.size[match(unique(data$school), data$school)], coefs$SES_c, xlab = "Class Size", ylab = "Slope for SES_c")
plot(data$class.size[match(unique(data$school), data$school)], coefs$IQ_c, xlab = "Class Size", ylab = "Slope for IQ_c")
# Fit the one-way random-effects ANOVA
model <- lmer(test ~ (1 | school), data = data)
vc <- VarCorr(model)
vc
icc <- 4.2743^2 / (4.2743^2 + sigma(model)^2)
icc
model <- lmer(test ~ SES_c + IQ_c + (1 + SES_c + IQ_c | school), data = data)
summary(model)
data <- read.table("Datasets/Snijders.txt", header = TRUE)
data <- na.omit(data)
data$SES_c <- with(data, ses - meanses)
data$IQ_c <- with(data, iq - meaniq)
schools <- sample(unique(data$school), 20)
par(mfrow = c(5, 4), mar = c(2.5, 2.5, 1, 1))
for (i in 1:length(schools)) {
school_data <- data[data$school == schools[i], ]
plot(school_data$SES_c, school_data$test, main = paste("School", schools[i]),
xlab = "SES_c", ylab = "Test Score")
abline(lm(test ~ SES_c, data = school_data))
plot(school_data$IQ_c, school_data$test, main = paste("School", schools[i]),
xlab = "IQ_c", ylab = "Test Score")
abline(lm(test ~ IQ_c, data = school_data))
}
model <- lmer(test ~ SES_c + IQ_c + (1 + SES_c + IQ_c | school), data=data)
coefs <- coef(model)$school
plot(data$meanses[match(unique(data$school), data$school)], coefs$`(Intercept)`, xlab = "Mean SES", ylab = "Intercept")
plot(data$meaniq[match(unique(data$school), data$school)], coefs$`(Intercept)`, xlab = "Mean IQ", ylab = "Intercept")
plot(data$class.size[match(unique(data$school), data$school)], coefs$`(Intercept)`, xlab = "Class Size", ylab = "Intercept")
plot(data$meaniq[match(unique(data$school), data$school)], coefs$SES_c, xlab = "Mean SES", ylab = "Slope for IQ_c")
plot(data$meanses[match(unique(data$school), data$school)], coefs$SES_c, xlab = "Class Size", ylab = "Slope for SES_c")
plot(data$meaniq[match(unique(data$school), data$school)], coefs$IQ_c, xlab = "Mean SES", ylab = "Slope for IQ_c")
plot(data$meanses[match(unique(data$school), data$school)], coefs$IQ_c, xlab = "Class Size", ylab = "Slope for SES_c")
plot(data$class.size[match(unique(data$school), data$school)], coefs$SES_c, xlab = "Class Size", ylab = "Slope for SES_c")
plot(data$class.size[match(unique(data$school), data$school)], coefs$IQ_c, xlab = "Class Size", ylab = "Slope for IQ_c")
# Fit the one-way random-effects ANOVA
model <- lmer(test ~ (1 | school), data = data)
vc <- VarCorr(model)
vc
icc <- 4.2743^2 / (4.2743^2 + sigma(model)^2)
icc
model <- lmer(test ~ SES_c + IQ_c + (1 + SES_c + IQ_c | school), data = data)
summary(model)
null_model <- lmer(test ~ SES_c + IQ_c + (1 | school), data = data)
anova(null_model, model)
null_model = lmer(test ~ SES_c+IQ_c + (1 + IQ_c| school), data = data)
anova(null_model, model)
null_model = lmer(test ~ SES_c + IQ_c + (1 + SES_c| school), data = data)
anova(null_model, model)
final_model = lmer(test ~ SES_c + IQ_c + (1 + IQ_c| school), data = data)
summary(final_model)
m2 <- lmer(test ~ SES_c + IQ_c + meanses + meaniq + class.size + IQ_c:meanses + IQ_c:meaniq + IQ_c * class.size +
(IQ_c | school) + (1 | school), data = data)
summary(m2)
m3 <- lmer(test ~ SES_c + IQ_c + meanses + meaniq + IQ_c:meanses + IQ_c:meaniq + (IQ_c | school) + (1 | school), data = data)
summary(m3)
final_model <- lmer(test ~ SES_c + IQ_c +  meaniq  + (IQ_c | school) + (1 | school), data = data)
summary(final_model)
data$high_pass <- ifelse(data$test > 52, 1, 0)
model <- glmer(high_pass ~ 1 + (1 | school), data = data)
vc <- VarCorr(model)
vc
icc <- 0.066921^2 / (0.066921^2 + sigma(model)^2)
icc
model <- glmer(high_pass ~ SES_c + IQ_c + (1 + SES_c + IQ_c | school), family = binomial, data = data)
S(model)
null_model <- glmer(high_pass ~ SES_c + IQ_c + (1 | school), family = binomial, data = data)
anova(model, null_model)
with_leve_two <- glmer(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size +  (1 | school), family = binomial, data = data)
null_model = glm(high_pass ~ SES_c + IQ_c + meanses + meaniq +
class.size , family = binomial, data = data)
anova(with_leve_two, null_model)
reduced = glmer(high_pass ~ SES_c + IQ_c + meaniq +
(1 | school), family = binomial, data = data)
anova(reduced, with_leve_two)
final_model = glmer(high_pass ~ SES_c + IQ_c + meaniq +
(1 | school),family = binomial,  data = data)
S(final_model)
phillips <- read.table("data/Phillips.txt", header=TRUE)
phillips <- read.table("Datasets/Phillips.txt", header=TRUE)
ggplot(phillips, aes(x=age.adjusted, y=body.fat, group=subject)) + geom_line()
phillips <- read.table("Datasets/Phillips.txt", header=TRUE)
ggplot(phillips, aes(x=age.adjusted, y=body.fat, group=subject)) + geom_line()
ggplot(phillips, aes(x=age.adjusted, y=body.fat, group=subject)) + geom_smooth(method="loess")
# Sample 30 girls
set.seed(353)
my_sample <- sample(unique(phillips$subject), 30)
for (gril in my_sample) {
temp_data <- phillips[phillips$subject == gril,]
plot(body.fat ~ age.adjusted, data=temp_data, type='l')
}
phillips <- read.table("Datasets/Phillips.txt", header=TRUE)
ggplot(phillips, aes(x=age.adjusted, y=body.fat, group=subject)) + geom_line()
ggplot(phillips, aes(x=age.adjusted, y=body.fat, group=subject)) + geom_smooth(method="loess")
# Sample 30 girls
my_sample <- sample(unique(phillips$subject), 30)
for (gril in my_sample) {
temp_data <- phillips[phillips$subject == gril,]
plot(body.fat ~ age.adjusted, data=temp_data, type='l')
}
